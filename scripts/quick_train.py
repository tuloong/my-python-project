#!/usr/bin/env python3
"""
å¿«é€Ÿè®­ç»ƒè„šæœ¬

ä¸€ä¸ªç®€åŒ–çš„è®­ç»ƒæµç¨‹ï¼Œå¿«é€ŸéªŒè¯æ¨¡å‹æ•ˆæœ
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.data.data_loader import DataLoader
from src.data.data_preprocessor import DataPreprocessor
from src.models.tree_models import XGBoostModel, LightGBMModel
from src.training.trainer import Trainer


def quick_train():
    """å¿«é€Ÿè®­ç»ƒæµç¨‹"""
    print("ğŸš€ å¿«é€Ÿè®­ç»ƒå¼€å§‹...")
    
    # 1. åŠ è½½æ•°æ®
    loader = DataLoader()
    data = loader.load_saved_data("train.csv", "raw")
    if data is None:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®")
        return
    
    # 2. ç®€å•é¢„å¤„ç†
    data = loader.rename_columns(data, loader.column_mapping)
    preprocessor = DataPreprocessor()
    data = preprocessor.clean_stock_data(data)
    data = preprocessor.handle_missing_values(data)
    
    # 3. æ·»åŠ åŸºç¡€ç‰¹å¾
    data = preprocessor.add_technical_indicators(data)
    data = preprocessor.calculate_returns(data, periods=[1])
    data = data.dropna()
    
    # 4. å‡†å¤‡æ•°æ®
    feature_cols = [col for col in data.columns if not col.startswith('return_')]
    X = data[feature_cols]
    y = data['return_1d']
    
    print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    
    # 5. è®­ç»ƒæ¨¡å‹
    trainer = Trainer()
    models = {
        'XGBoost': XGBoostModel(),
        'LightGBM': LightGBMModel()
    }
    
    trained = trainer.train_multiple_models(models, X, y)
    
    # 6. æ˜¾ç¤ºç»“æœ
    print("\nğŸ“ˆ è®­ç»ƒç»“æœ:")
    for name, model in trained.items():
        metrics = model.evaluate(X, y)
        print(f"   {name}: RMSE={metrics['rmse']:.4f}, RÂ²={metrics['r2']:.4f}")
    
    # 7. ä¿å­˜æœ€ä½³æ¨¡å‹
    trainer.save_models('outputs/models')
    print("âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜")


if __name__ == "__main__":
    quick_train()