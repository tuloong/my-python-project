# è‚¡ç¥¨é¢„æµ‹é¡¹ç›® - ä¸‹ä¸€æ­¥æ“ä½œæŒ‡å—

## ğŸ¯ é¡¹ç›®çŠ¶æ€
âœ… **æ•°æ®æå–åŠŸèƒ½å·²å®Œæˆ**
âœ… **æ¨¡å‹è®­ç»ƒæ¡†æ¶å·²å®Œå–„**
âœ… **å¤šç§é¢„æµ‹æ¨¡å‹å·²å®ç°**

## ğŸ“‹ å·²å®ç°çš„æ¨¡å‹

### 1. ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
- **æ ‘æ¨¡å‹**: XGBoost, LightGBM, CatBoost, ExtraTrees
- **çº¿æ€§æ¨¡å‹**: Linear, Ridge, Lasso, ElasticNet, BayesianRidge, Polynomial
- **é›†æˆæ¨¡å‹**: Voting, Stacking, Blending, Bagging, AdaBoost

### 2. æ·±åº¦å­¦ä¹ æ¨¡å‹
- **LSTM**: é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ
- **GRU**: é—¨æ§å¾ªç¯å•å…ƒ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1: å¿«é€Ÿè®­ç»ƒï¼ˆæ¨èï¼‰
```bash
# è¿è¡Œå¿«é€Ÿè®­ç»ƒè„šæœ¬
python scripts/quick_train.py
```

### æ–¹æ³•2: å®Œæ•´è®­ç»ƒæµç¨‹
```bash
# è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹
python scripts/train_complete.py
```

### æ–¹æ³•3: è‡ªå®šä¹‰è®­ç»ƒ
```python
from src.data.data_loader import DataLoader
from src.data.data_preprocessor import DataPreprocessor
from src.models.tree_models import XGBoostModel
from src.training.trainer import Trainer

# 1. åŠ è½½æ•°æ®
loader = DataLoader()
data = loader.load_saved_data("train.csv", "raw")

# 2. é¢„å¤„ç†
preprocessor = DataPreprocessor()
data = preprocessor.clean_stock_data(data)
data = preprocessor.add_technical_indicators(data)
data = data.dropna()

# 3. å‡†å¤‡æ•°æ®
X = data.drop(columns=['close'])
y = data['close']

# 4. è®­ç»ƒæ¨¡å‹
trainer = Trainer()
model = XGBoostModel()
trained_model = trainer.train_single_model(model, X, y)

# 5. è¯„ä¼°
metrics = trained_model.evaluate(X, y)
print(f"RMSE: {metrics['rmse']:.4f}, RÂ²: {metrics['r2']:.4f}")
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### è®­ç»ƒLSTMæ¨¡å‹
```python
from src.models.lstm_model import LSTMModel

# å‡†å¤‡åºåˆ—æ•°æ®
lstm_model = LSTMModel({
    'seq_length': 30,
    'lstm_units': [64, 32],
    'epochs': 50
})

lstm_model.fit(data)
predictions = lstm_model.predict(data)
```

### è®­ç»ƒé›†æˆæ¨¡å‹
```python
from src.models.ensemble_models import StackingEnsemble

ensemble = StackingEnsemble()
ensemble.add_model(XGBoostModel())
ensemble.add_model(LightGBMModel())
ensemble.add_model(RidgeModel())

ensemble.fit(X, y)
```

## ğŸ“Š æ¨¡å‹é…ç½®æ–‡ä»¶

ç¼–è¾‘ `configs/model_config.yaml` æ¥è‡ªå®šä¹‰æ¨¡å‹å‚æ•°ï¼š

```yaml
xgboost:
  n_estimators: 200
  max_depth: 8
  learning_rate: 0.1

lstm:
  seq_length: 60
  lstm_units: [128, 64]
  epochs: 100
```

## ğŸ“ è¾“å‡ºæ–‡ä»¶

è®­ç»ƒå®Œæˆåï¼Œæ–‡ä»¶å°†ä¿å­˜åˆ°ï¼š
- `outputs/models/` - è®­ç»ƒå¥½çš„æ¨¡å‹
- `outputs/plots/` - å¯è§†åŒ–å›¾è¡¨
- `outputs/predictions/` - é¢„æµ‹ç»“æœ

## ğŸ› ï¸ ä¾èµ–æ£€æŸ¥

è¿è¡Œå‰è¯·ç¡®ä¿å·²å®‰è£…ï¼š
```bash
pip install tensorflow
pip install xgboost
pip install lightgbm
pip install catboost
```

## ğŸ“ ä¸‹ä¸€æ­¥å»ºè®®

1. **è¿è¡Œå¿«é€Ÿè®­ç»ƒ**éªŒè¯ç³»ç»Ÿå®Œæ•´æ€§
2. **è°ƒæ•´æ¨¡å‹å‚æ•°**ä¼˜åŒ–æ€§èƒ½
3. **æ·»åŠ æ›´å¤šç‰¹å¾**æå‡é¢„æµ‹å‡†ç¡®æ€§
4. **å°è¯•ä¸åŒæ¨¡å‹**å¯¹æ¯”æ•ˆæœ
5. **éƒ¨ç½²é¢„æµ‹æœåŠ¡**ç”¨äºå®é™…åº”ç”¨

## ğŸ“ é‡åˆ°é—®é¢˜ï¼Ÿ

æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ï¼š`logs/training.log`
æˆ–è”ç³»é¡¹ç›®ç»´æŠ¤äººå‘˜è·å–æ”¯æŒã€‚